{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 5m_12candles_strong_2016-2018.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferzok/PatternDiscovery/blob/master/Copy_of_5m_12candles_strong_2016_2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajViVkNHBmlP",
        "colab_type": "code",
        "outputId": "df735562-35a3-42fd-aeb0-400f55291fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/5m.Si_160101_180930.csv',delimiter=';')  \n",
        "dataset=dataset.rename(columns={ dataset.columns[0]: \"date\" })\n",
        "dataset['date'] =  pd.to_datetime(dataset['date'], format='%Y%m%d')\n",
        "dataset=dataset.rename(columns={ dataset.columns[1]: \"time\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[2]: \"open\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[3]: \"high\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[4]: \"low\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[5]: \"close\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[6]: \"vol\" })\n",
        "dataset['weekday'] = dataset['date'].dt.dayofweek\n",
        "# next 12 periods for 100 points\n",
        "k = 12\n",
        "points=100\n",
        "res = dataset.high.rolling(k).max().shift(-k)\n",
        "dataset['NextHigh'+str(points)]=res\n",
        "res = dataset.low.rolling(k).min().shift(-k)\n",
        "dataset['NextLow'+str(points)]=res\n",
        "dataset['prUp'+str(points)]=dataset.loc[ : , 'NextHigh'+str(points)]-dataset.high-points>0\n",
        "dataset['prDn'+str(points)]=dataset.low-dataset.loc[ : , 'NextLow'+str(points)]-points>0\n",
        "# next 96 periods for 300 points\n",
        "k = 96\n",
        "points=300\n",
        "res = dataset.high.rolling(k).max().shift(-k)\n",
        "dataset['NextHigh'+str(points)]=res\n",
        "res = dataset.low.rolling(k).min().shift(-k)\n",
        "dataset['NextLow'+str(points)]=res\n",
        "dataset['prUp'+str(points)]=(dataset.loc[ : , 'NextHigh'+str(points)]-dataset.high-points>0) & (dataset.low-dataset.loc[ : , 'NextLow'+str(points)]-points<0)\n",
        "dataset['prDn'+str(points)]=(dataset.loc[ : , 'NextHigh'+str(points)]-dataset.high-points<0) & (dataset.low-dataset.loc[ : , 'NextLow'+str(points)]-points>0)\n",
        "dataset['date'] = dataset['date'].dt.dayofweek# end\n",
        "# dataset = dataset.drop(dataset[dataset.time > 230000].index)\n",
        "t=12\n",
        "nrows=dataset.shape[0]\n",
        "df=pd.DataFrame({'time': dataset.iloc[t-1:nrows,1]})\n",
        "df['weekday']=dataset.iloc[t-1:nrows,7].values\n",
        "for i in range(0, t):\n",
        "   df['HL'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,4].values\n",
        "   df['HL'+str(i)]=sc.fit_transform(df['HL'+str(i)])\n",
        "   df['HO'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,2].values\n",
        "   df['HC'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,5].values\n",
        "   df['V'+str(i)]=dataset.iloc[t-1-i:nrows-i,6].values\n",
        "   df['C'+str(i)]=dataset.iloc[t-1-i:nrows-i,5].values\n",
        "df['pr100Up']=dataset.iloc[t-1:nrows,10].values\n",
        "df['pr100Dn']=dataset.iloc[t-1:nrows,11].values\n",
        "df['pr300Up']=dataset.iloc[t-1:nrows,14].values\n",
        "df['pr300Dn']=dataset.iloc[t-1:nrows,15].values\n",
        "df = df.drop(df[df.time > 230000].index)\n",
        "df = df.drop(df[df.time < 110000].index)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "numberinputs=df.shape[1]-4\n",
        "X = df.iloc[:, 0:numberinputs].values\n",
        "y_UP100 = df.iloc[:, numberinputs].values\n",
        "y_DOWN100 = df.iloc[:, numberinputs+1].values\n",
        "y_UP300 = df.iloc[:, numberinputs+2].values\n",
        "y_DOWN300 = df.iloc[:, numberinputs+3].values\n",
        "\n",
        "# Encoding categorical data\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train_DOWN100, y_test_DOWN100 = train_test_split(X, y_DOWN100, test_size = 0.2, random_state = 0)\n",
        "X_train, X_test, y_train_UP100, y_test_UP100 = train_test_split(X, y_UP100, test_size = 0.2, random_state = 0)\n",
        "\n",
        "X_train, X_test, y_train_UP300, y_test_UP300 = train_test_split(X, y_UP300, test_size = 0.2, random_state = 0)\n",
        "X_train, X_test, y_train_DOWN300, y_test_DOWN300 = train_test_split(X, y_DOWN300, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Part 2 - Now let's make the ANN!\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Initialising the ANN\n",
        "classifierUP100 = Sequential()\n",
        "classifierDN100 = Sequential()\n",
        "classifierUP300 = Sequential()\n",
        "classifierDN300 = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "classifierUP100.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = numberinputs))\n",
        "classifierDN100.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = numberinputs))\n",
        "classifierUP300.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = numberinputs))\n",
        "classifierDN300.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = numberinputs))\n",
        "\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifierUP100.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "\n",
        "classifierDN100.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "classifierUP300.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "\n",
        "classifierDN300.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifierUP100.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "classifierDN100.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "classifierUP300.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "classifierDN300.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "\n",
        "# Compiling the ANN\n",
        "classifierUP100.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "classifierDN100.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "classifierUP300.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "classifierDN300.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "classifierUP100.fit(X_train, y_train_UP100, batch_size = 128, epochs =50)\n",
        "classifierDN100.fit(X_train, y_train_DOWN100, batch_size = 1024, epochs = 50)\n",
        "classifierUP300.fit(X_train, y_train_UP300, batch_size = 1024, epochs = 50)\n",
        "classifierDN300.fit(X_train, y_train_DOWN300, batch_size = 1024, epochs = 50)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "model_jsonup100 = classifierUP100.to_json()\n",
        "with open('classifierUP100.json', \"w\") as json_file:\n",
        "    json_file.write(model_jsonup100)\n",
        "json_file.close\n",
        "files.download('classifierUP100.json')\n",
        "# serialize weights to HDF5\n",
        "classifierUP100.save_weights('classifierUP100.h5')\n",
        "files.download('classifierUP100.h5')\n",
        "\n",
        "\n",
        "model_json_dn100 = classifierDN100.to_json()\n",
        "with open('classifierDN100.json', \"w\") as json_file:\n",
        "    json_file.write(model_json_dn100)\n",
        "json_file.close\n",
        "files.download('classifierDN100.json')\n",
        "# serialize weights to HDF5\n",
        "classifierDN100.save_weights('classifierDN100.h5')\n",
        "files.download('classifierDN100.h5')\n",
        "\n",
        "\n",
        "model_jsonUP300 = classifierUP300.to_json()\n",
        "with open('classifierUP300.json', \"w\") as json_file:\n",
        "    json_file.write(model_jsonUP300)\n",
        "json_file.close\n",
        "files.download('classifierUP300.json')\n",
        "# serialize weights to HDF5\n",
        "classifierUP300.save_weights('classifierUP300.h5')\n",
        "files.download('classifierUP300.h5')\n",
        "\n",
        "\n",
        "model_jsonDN300 = classifierDN300.to_json()\n",
        "with open('classifierDN300.json', \"w\") as json_file:\n",
        "    json_file.write(model_jsonDN300)\n",
        "files.download('classifierDN300.json')\n",
        "# serialize weights to HDF5\n",
        "classifierDN300.save_weights('classifierDN300.h5')\n",
        "files.download('classifierDN300.h5')\n",
        "print(\"Saved model to drive\")\n",
        "\n",
        "# Predicting the Test set results\n",
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred_DOWN100 = classifierDN100.predict(X_test)\n",
        "y_pred_DOWN100 = (y_pred_DOWN100 >0.7)\n",
        "cm_DOWN100 = confusion_matrix(y_test_DOWN100, y_pred_DOWN100)\n",
        "print(cm_DOWN100)\n",
        "\n",
        "y_pred_UP100 = classifierUP100.predict(X_test)\n",
        "y_pred_UP100 = (y_pred_UP100 >0.8)\n",
        "cm_UP100 = confusion_matrix(y_test_UP100, y_pred_UP100)\n",
        "print(cm_UP100)\n",
        "\n",
        "y_pred_DOWN300 = classifierDN300.predict(X_test)\n",
        "y_pred_DOWN300 = (y_pred_DOWN300> 0.7)\n",
        "cm_DOWN300 = confusion_matrix(y_test_DOWN300, y_pred_DOWN300)\n",
        "\n",
        "y_pred_UP300 = classifierUP300.predict(X_test)\n",
        "y_pred_UP300 = (y_pred_UP300 >0.7)\n",
        "cm_UP300 = confusion_matrix(y_test_UP300, y_pred_UP300)\n",
        "nrows=y_test_UP100.shape[0]\n",
        "allresults=pd.DataFrame({'time': df.iloc[0:nrows,0]})\n",
        "#allresults['date']=df.iloc[0:nrows,6] \n",
        "allresults['Price']=df.iloc[0:nrows,6] \n",
        "allresults['UP100']=y_pred_UP100 \n",
        "allresults['UP100Real']=y_test_UP100 \n",
        "allresults['DN100']=y_pred_DOWN100 \n",
        "allresults['DN100Real']=y_test_DOWN100 \n",
        "allresults['UP300']=y_pred_UP300 \n",
        "allresults['UP300Real']=y_test_UP300 \n",
        "allresults['DN300']=y_pred_DOWN300 \n",
        "allresults['DN300Real']=y_test_DOWN300 \n",
        "from google.colab import files\n",
        "allresults.to_csv('full_results.csv', sep=';')\n",
        "files.download('full_results.csv')\n",
        "\n",
        "\n",
        "#CHECK current results\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Colab Notebooks/SPFB.Si_181112_181112.csv',delimiter=';')  \n",
        "dataset=dataset.rename(columns={ dataset.columns[0]: \"date\" })\n",
        "dataset['date'] =  pd.to_datetime(dataset['date'], format='%Y%m%d')\n",
        "dataset=dataset.rename(columns={ dataset.columns[1]: \"time\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[2]: \"open\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[3]: \"high\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[4]: \"low\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[5]: \"close\" })\n",
        "dataset=dataset.rename(columns={ dataset.columns[6]: \"vol\" })\n",
        "dataset['weekday'] = dataset['date'].dt.dayofweek\n",
        "# next 24 periods for 100 points\n",
        "k = 12\n",
        "points=100\n",
        "res = dataset.high.rolling(k).max().shift(-k)\n",
        "dataset['NextHigh'+str(points)]=res\n",
        "res = dataset.low.rolling(k).min().shift(-k)\n",
        "dataset['NextLow'+str(points)]=res\n",
        "dataset['prUp'+str(points)]=dataset.loc[ : , 'NextHigh'+str(points)]-dataset.close-points>0\n",
        "dataset['prDn'+str(points)]=dataset.close-dataset.loc[ : , 'NextLow'+str(points)]-points>0\n",
        "# next 96 periods for 300 points\n",
        "k = 96\n",
        "points=300\n",
        "res = dataset.high.rolling(k).max().shift(-k)\n",
        "dataset['NextHigh'+str(points)]=res\n",
        "res = dataset.low.rolling(k).min().shift(-k)\n",
        "dataset['NextLow'+str(points)]=res\n",
        "dataset['prUp'+str(points)]=dataset.loc[ : , 'NextHigh'+str(points)]-dataset.close-points>0\n",
        "dataset['prDn'+str(points)]=dataset.close-dataset.loc[ : , 'NextLow'+str(points)]-points>0\n",
        "# end\n",
        "dataset = dataset.drop(dataset[dataset.time > 230000].index)\n",
        "t=12\n",
        "nrows=dataset.shape[0]\n",
        "dfx=pd.DataFrame({'time': dataset.iloc[t-1:nrows,1]})\n",
        "dfx['weekday']=dataset.iloc[t-1:nrows,7].values\n",
        "for i in range(0, t):\n",
        "   dfx['HL'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,4].values\n",
        "   dfx['HO'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,2].values\n",
        "   dfx['HC'+str(i)]=dataset.iloc[t-1-i:nrows-i,3].values-dataset.iloc[t-1-i:nrows-i,5].values\n",
        "   dfx['V'+str(i)]=dataset.iloc[t-1-i:nrows-i,6].values\n",
        "   dfx['С'+str(i)]=dataset.iloc[t-1-i:nrows-i,5].values\n",
        "dfx['pr100Up']=dataset.iloc[t-1:nrows,10].values\n",
        "dfx['pr100Dn']=dataset.iloc[t-1:nrows,11].values\n",
        "dfx['pr300Up']=dataset.iloc[t-1:nrows,14].values\n",
        "dfx['pr300Dn']=dataset.iloc[t-1:nrows,15].values\n",
        "dfx = dfx.reset_index(drop=True)\n",
        "dfxrows=dfx.shape[0]\n",
        "Xx = dfx.iloc[:, 0:numberinputs].values  \n",
        "sc = StandardScaler()\n",
        "Xx = sc.fit_transform(Xx)\n",
        "y_pred_UP100 = classifierUP100.predict(Xx) \n",
        "y_pred_DOWN100 = classifierDN100.predict(Xx)\n",
        "y_pred_UP300 = classifierUP300.predict(Xx) \n",
        "y_pred_DOWN300 = classifierDN300.predict(Xx)\n",
        "\n",
        "allresults=pd.DataFrame({'time': dfx.iloc[0:nrows,0]})\n",
        "allresults['Price']=dfx.iloc[0:nrows,6] \n",
        "allresults['UP100']=y_pred_UP100 \n",
        "allresults['DN100']=y_pred_DOWN100 \n",
        "allresults['UP300']=y_pred_UP300 \n",
        "allresults['DN300']=y_pred_DOWN300 \n",
        "\n",
        "from google.colab import files\n",
        "allresults.to_csv('60min_results.csv', sep=';')\n",
        "files.download('60min_results.csv')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11         97.0\n",
              "12        170.0\n",
              "13        131.0\n",
              "14        154.0\n",
              "15        187.0\n",
              "16        150.0\n",
              "17        150.0\n",
              "18        111.0\n",
              "19        154.0\n",
              "20         88.0\n",
              "21        110.0\n",
              "22        202.0\n",
              "23         54.0\n",
              "24         60.0\n",
              "25        115.0\n",
              "26         73.0\n",
              "27        163.0\n",
              "28         81.0\n",
              "29        148.0\n",
              "30        155.0\n",
              "31        151.0\n",
              "32        115.0\n",
              "33         56.0\n",
              "34        108.0\n",
              "35         64.0\n",
              "36         63.0\n",
              "37        146.0\n",
              "38         69.0\n",
              "39        122.0\n",
              "40        163.0\n",
              "          ...  \n",
              "111868     17.0\n",
              "111869     17.0\n",
              "111870     22.0\n",
              "111871     20.0\n",
              "111872     26.0\n",
              "111873     48.0\n",
              "111874     22.0\n",
              "111875     16.0\n",
              "111876     20.0\n",
              "111877     40.0\n",
              "111878     18.0\n",
              "111879     18.0\n",
              "111880     28.0\n",
              "111881     21.0\n",
              "111882     62.0\n",
              "111883     30.0\n",
              "111884     73.0\n",
              "111885     25.0\n",
              "111886     20.0\n",
              "111887     45.0\n",
              "111888     36.0\n",
              "111889     39.0\n",
              "111890     31.0\n",
              "111891      9.0\n",
              "111892     52.0\n",
              "111893     67.0\n",
              "111894     34.0\n",
              "111895     21.0\n",
              "111896     25.0\n",
              "111897     37.0\n",
              "Name: HL0, Length: 111887, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}